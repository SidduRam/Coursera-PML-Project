mybin[4]
mybin[3]
mybin[2]
mybin[1]
mybin[7]
mybin[7]  mybin[8]
mybin[8]
mybin[0]
mybin[1]
mybin[2]
info()
skip()
getwd()
rm(list = ls())
library(UsingR);
data(mtcars)
mtcars[] <- lapply(mtcars, as.integer)
x1 <- mtcars$weight
y1 <- factor(mtcars$mpg)
fitcar <- lm(y1~x1,na.action=na.exclude)
library(UsingR);
data(mtcars)
mtcars[] <- lapply(mtcars, as.integer)
x1 <- mtcars$weight
y1 <- factor(mtcars$mpg)
fitcar <- lm(y1~x1,na.action=na.exclude)
x <- c(0.61, 0.93, 0.83, 0.35, 0.54, 0.16, 0.91, 0.62, 0.62)
y <- c(0.67, 0.84, 0.6, 0.18, 0.85, 0.47, 1.1, 0.65, 0.36)
fit <- lm(y ~ x)
summary(fit)
data(mtcars)
#mtcars[] <- lapply(mtcars, as.integer)
x1 <- mtcars$weight
y1 <- factor(mtcars$mpg)
fitcar <- lm(y1~x1,na.action=na.exclude)
lm(y1~x1,na.action=na.exclude)
x1 <- mtcars$wt
y1 <- factor(mtcars$mpg)
fitcar <- lm(y1~x1,na.action=na.exclude)
sumCoef <- summary(fitcar)$coefficients
sumCoef[1,1] + c(-1, 1) * qt(.975, df = fitcar$df) * sumCoef[1, 2]
sumCoef[2,1] + c(-1, 1) * qt(.975, df = fitcar$df) * sumCoef[2, 2]
mtcars[] <- lapply(mtcars, as.integer)
mtcars[] <- lapply(mtcars, as.integer)
x1 <- mtcars$wt
y1 <- factor(mtcars$mpg)
fitcar <- lm(y1~x1,na.action=na.exclude)
sumCoef <- summary(fitcar)$coefficients
sumCoef[1,1] + c(-1, 1) * qt(.975, df = fitcar$df) * sumCoef[1, 2]
sumCoef[2,1] + c(-1, 1) * qt(.975, df = fitcar$df) * sumCoef[2, 2]
x1 <- mtcars$wt
y1 <- mtcars$mpg
fitcar <- lm(y1~x1,na.action=na.exclude)
sumCoef <- summary(fitcar)$coefficients
sumCoef
sumCoef[1,1] + c(-1, 1) * qt(.975, df = fitcar$df) * sumCoef[1, 2]
sumCoef[2,1] + c(-1, 1) * qt(.975, df = fitcar$df) * sumCoef[2, 2]
predict(fitcar,data.frame(x1=mean(x1)), interval="confidence")
sumCoef
sumCoef[1,1] + c(-1, 1) * qt(.975, df = fitcar$df) * sumCoef[1, 2]
sumCoef[2,1] + c(-1, 1) * qt(.975, df = fitcar$df) * sumCoef[2, 2]
mean(x1)
predict(fitcar,data.frame(x1=mean(x1)), interval="confidence")
help(mtcars)
p1<-predict(fitcar,data.frame(x1), interval="confidence")
plot(x,y,xlab='Weight (1000lb)',ylab='MPG')
p1<-predict(fitcar,data.frame(x1), interval="confidence")
plot(x1,y1,xlab='Weight (1000lb)',ylab='MPG')
p1<-predict(fitcar,data.frame(x1), interval="confidence")
plot(x1,y1,xlab='Weight (1000lb)',ylab='MPG')
abline(fitcar,col="red")
lines(x1,p1[,2],col="purple")
lines(x1,p1[,3],col="purple")
p1<-predict(fitcar,data.frame(x1), interval="confidence")
plot(x1,y1,xlab='Weight (1000lb)',ylab='MPG')
abline(fitcar,col="red")
lines(x1,p1[,2],col="purple")
lines(x1,p1[,3],col="purple")
p1<-predict(fitcar,data.frame(x1), interval="confidence")
plot(x1,y1,xlab='Weight (1000lb)',ylab='MPG')
abline(fitcar,col="red")
lines(x1,p1[,2],col="purple")
lines(x1,p1[,3],col="purple")
help(mtcars)
predict(fitcar,data.frame(x1=mean(x1)), interval="prediction")
predict(fitcar,data.frame(x=3), interval="prediction")
predict(fitcar,data.frame(3), interval="prediction")
predict(fitcar,data.frame(x1=3), interval="prediction")
predict(fitcar,data.frame(x1=3), interval="prediction")
predict(fitcar,data.frame(x1=3), interval="prediction")
predict(fitcar,data.frame(x1=3), interval="prediction")
predict(fitcar,data.frame(x1=3), interval="prediction")
fitcar <- lm(y1~x1,na.action=na.exclude)
fitcar1 <- lm(y1~x1)
summary(fitcar)
summary(fitcar1)
predict(fitcar1,data.frame(x1=3), interval="prediction")
rm(ls=())
?rm
rm(list=ls=())
rm(list=ls())
library(UsingR);
data(mtcars)
mtcars[] <- lapply(mtcars, as.integer)
x <- mtcars$wt
y <- mtcars$mpg
fitcar <- lm(y~x,na.action=na.exclude)
sumCoef <- summary(fitcar)$coefficients
sumCoef
sumCoef[1,1] + c(-1, 1) * qt(.975, df = fitcar$df) * sumCoef[1, 2]
sumCoef[2,1] + c(-1, 1) * qt(.975, df = fitcar$df) * sumCoef[2, 2]
# Another approach
predict(fitcar,data.frame(x=mean(x)), interval="confidence")
p1<-predict(fitcar,data.frame(x1), interval="confidence")
plot(x,y,xlab='Weight (1000lb)',ylab='MPG')
abline(fitcar,col="red")
lines(x,p1[,2],col="purple")
lines(x,p1[,3],col="purple")
p1<-predict(fitcar,data.frame(x), interval="confidence")
plot(x,y,xlab='Weight (1000lb)',ylab='MPG')
abline(fitcar,col="red")
lines(x,p1[,2],col="purple")
lines(x,p1[,3],col="purple")
fit <- lm(y~x)
predict(fit,data.frame(x=3), interval="prediction")
fit1 <- lm(y~I(x/2)
fit1 <- lm(y~I(x/2))
summary(fit1)
x <- mtcars$wt
y <- mtcars$mpg
fitcar <- lm(y~x,na.action=na.exclude)
fit1 <- lm(y~I(x/2))
summary(fit1)
predict(fit1,data.frame(x=mean(x/2)), interval="confidence")
predict(fit1,data.frame(x/2), interval="confidence")
tbl1<-summary(fit1)$coefficients
mn<-tbl1[2,1]      #mean is the estimated slope
std_err<-tbl1[2,2] #standard error
deg_fr<-fit1$df    #degree of freedom
#Two sides T-Tests
mn + c(-1,1) * qt(0.975,df=deg_fr) * std_err
fit1 <- lm(y~I(x/2))
summary(fit1)
tbl1<-summary(fit1)$coefficients
mn<-tbl1[2,1]      #mean is the estimated slope
std_err<-tbl1[2,2] #standard error
deg_fr<-fit1$df    #degree of freedom
tbl1
mn<-tbl1[2,1]      #mean is the estimated slope
std_err<-tbl1[2,2] #standard error
deg_fr<-fit1$df    #degree of freedom
mn
str_err
std_err
deg_fr
mn + c(-1,1) * qt(0.975,df=deg_fr) * std_err
summary(fit)$coefficients
fit3<-lm(y~I(x/100))
summary(fit3)$coefficients
fitRes <- fit$residuals ^ 2
fitIntercept <- lm(mpg ~ 1, mtcars)
fitInterceptRes <- fitIntercept$residuals ^ 2
fitInterceptRes <- fitIntercept$residuals ^ 2
sum(fitRes) /sum(fitInterceptRes) # 0.2471672
sum(resid(fit))
x <- c(0.61, 0.93, 0.83, 0.35, 0.54, 0.16, 0.91, 0.62, 0.62)
y <- c(0.67, 0.84, 0.6, 0.18, 0.85, 0.47, 1.1, 0.65, 0.36)
fit <- lm(y ~ x)
summary(fit)
x <- c(0.61, 0.93, 0.83, 0.35, 0.54, 0.16, 0.91, 0.62, 0.62)
y <- c(0.67, 0.84, 0.6, 0.18, 0.85, 0.47, 1.1, 0.65, 0.36)
fit <- lm(y ~ x)
summary(fit)
e <- resid(fit)
summary(e)
sqrt(deviance(fit)/df.residual(fit))
predict(fitcar,data.frame(x=mean(x)), interval="confidence")
p1<-predict(fitcar,data.frame(x), interval="confidence")
plot(x,y,xlab='Weight (1000lb)',ylab='MPG')
abline(fitcar,col="red")
lines(x,p1[,2],col="purple")
lines(x,p1[,3],col="purple")
library(UsingR);
data(mtcars)
mtcars[] <- lapply(mtcars, as.integer)
x <- mtcars$wt
y <- mtcars$mpg
fitcar <- lm(y~x,na.action=na.exclude)
sumCoef <- summary(fitcar)$coefficients
sumCoef
sumCoef[1,1] + c(-1, 1) * qt(.975, df = fitcar$df) * sumCoef[1, 2]
sumCoef[2,1] + c(-1, 1) * qt(.975, df = fitcar$df) * sumCoef[2, 2]
# Another approach
predict(fitcar,data.frame(x=mean(x)), interval="confidence")
# Another approach
predict(fitcar,data.frame(x=mean(x)), interval="confidence")
# A new car is coming weighing 3000 (weight (1,000 lbs))
predict(fit,data.frame(x=3), interval="prediction")
fit <- lm(y~x)
# A new car is coming weighing 3000 (weight (1,000 lbs))
predict(fit,data.frame(x=3), interval="prediction")
fit1 <- lm(y~I(x/2))
summary(fit1)
tbl1<-summary(fit1)$coefficients
mn<-tbl1[2,1]      #mean is the estimated slope
std_err<-tbl1[2,2] #standard error
deg_fr<-fit1$df    #degree of freedom
#Two sides T-Tests
mn + c(-1,1) * qt(0.975,df=deg_fr) * std_err
setwd("C:/Data-Science/Machine Learning")
training <- read.csv(url("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"), header = FALSE)
head(training)
summary(training)
qplot(classe, user_name, data=training)
library(ggplot2);
library(caret);
qplot(classe, user_name, data=training)
qplot(Classe, user_name, data=training)
qplot(classe,user_name,data=training)
qplot(V160,user_name,data=training)
qplot(V160,V2,data=training)
qplot(V160,V3, colour=V2,data=training)
qplot(V160,V4, colour=V2,data=training)
qplot(V160,V5, colour=V2,data=training)
nsv <- nearZeroVar(training, saveMetrics=TRUE)
nsv
M <- abs(cor(training))
diag(M) <- 0
which(M > 0.8, arr.ind=T)
M <- abs(cor(training[,-2]))
diag(M) <- 0
which(M > 0.8, arr.ind=T)
if (!file.exists("./Data/pml-training.csv")){
download.file("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv",
"./Data/pml-training.csv")
}
if (!file.exists("./Data/pml-testing.csv")){
download.file("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv",
"./Data/pml-testing.csv")
}
trainingdata = read.csv("./Data/pml-training.csv", na.strings = c("NA", ""))
dim(data)
trainingdata = read.csv("/Data/pml-training.csv", na.strings = c("NA", ""))
dim(data)
trainingdata = read.csv("./Data/pml-training.csv", na.strings = c("NA", ""))
dim(trainingdata)
summary(trainingdata)
summary(trainingdata$classe)
inTrain <- createDataPartition(y=trainingdata$classe, p=0.6, list=FALSE)
projTraining <- trainingdata[inTrain, ]; projTesting <- trainingdata[-inTrain, ]
dim(projTraining); dim(projTesting)
nsv <- nearZeroVar(projTraining, saveMetrics=TRUE)
nsv
nsv <- nearZeroVar(projTraining, saveMetrics=TRUE)
nsvVars <- names(projTraining) %in% c("new_window", "kurtosis_roll_belt", "kurtosis_picth_belt",
"kurtosis_yaw_belt", "skewness_roll_belt", "skewness_roll_belt.1", "skewness_yaw_belt",
"max_yaw_belt", "min_yaw_belt", "amplitude_yaw_belt", "avg_roll_arm", "stddev_roll_arm",
"var_roll_arm", "avg_pitch_arm", "stddev_pitch_arm", "var_pitch_arm", "avg_yaw_arm",
"stddev_yaw_arm", "var_yaw_arm", "kurtosis_roll_arm", "kurtosis_picth_arm",
"kurtosis_yaw_arm", "skewness_roll_arm", "skewness_pitch_arm", "skewness_yaw_arm",
"max_roll_arm", "min_roll_arm", "min_pitch_arm", "amplitude_roll_arm", "amplitude_pitch_arm",
"kurtosis_roll_dumbbell", "kurtosis_picth_dumbbell", "kurtosis_yaw_dumbbell", "skewness_roll_dumbbell",
"skewness_pitch_dumbbell", "skewness_yaw_dumbbell", "max_yaw_dumbbell", "min_yaw_dumbbell",
"amplitude_yaw_dumbbell", "kurtosis_roll_forearm", "kurtosis_picth_forearm", "kurtosis_yaw_forearm",
"skewness_roll_forearm", "skewness_pitch_forearm", "skewness_yaw_forearm", "max_roll_forearm",
"max_yaw_forearm", "min_roll_forearm", "min_yaw_forearm", "amplitude_roll_forearm",
"amplitude_yaw_forearm", "avg_roll_forearm", "stddev_roll_forearm", "var_roll_forearm",
"avg_pitch_forearm", "stddev_pitch_forearm", "var_pitch_forearm", "avg_yaw_forearm",
"stddev_yaw_forearm", "var_yaw_forearm")
projTraining <- projTraining[!nsvVars]
#
dim(projTraining)
projTraining <- projTraining[c(-1)]
subprojTraining <- projTraining #creating another subset to iterate in loop
for(i in 1:length(projTraining)) { #for every column in the training dataset
if( sum( is.na( projTraining[, i] ) ) /nrow(projTraining) >= .6 ) { #if n?? NAs > 60% of total observations
for(j in 1:length(subprojTraining)) {
if( length( grep(names(projTraining[i]), names(subprojTraining)[j]) ) ==1)  { #if the columns are the same:
subprojTraining <- subprojTraining[ , -j] #Remove that column
}
}
}
}
#To check the new N?? of observations
dim(subprojTraining)
subprojTraining <- projTraining #creating another subset to iterate in loop
for(i in 1:length(projTraining)) { #for every column in the training dataset
if( sum( is.na( projTraining[, i] ) ) /nrow(projTraining) >= .6 ) { # NAs > 60% of total observations)
for(j in 1:length(subprojTraining)) {
if( length( grep(names(projTraining[i]), names(subprojTraining)[j]) ) ==1)  { #if the columns are the same:
subprojTraining <- subprojTraining[ , -j] #Remove that column
}
}
}
}
#To check the new N?? of observations
dim(subprojTraining)
testingdata = read.csv("./Data/pml-testing.csv", na.strings = c("NA", ""))
clean1 <- colnames(subprojTraining)
clean2 <- colnames(subprojTraining[, -58]) #already with classe column removed
projTesting <- projTesting[clean1]
testing <- testingdata[clean2]
dim(projTesting)
dim(testing)
nzv <- nearZeroVar(trainingdata,saveMetrics=TRUE)
trainingdata <- trainingdata[,nzv$nzv==FALSE]
nzv <- nearZeroVar(testingdata,saveMetrics=TRUE)
testingdata <- testingdata[,nzv$nzv==FALSE]
# Partioning the training set into two
inTrain <- createDataPartition(y=trainingdata$classe, p=0.6, list=FALSE)
projTraining <- trainingdata[inTrain, ]; projTesting <- trainingdata[-inTrain, ]
dim(projTraining); dim(projTesting)
# Transformation 2: Killing first column of Dataset - ID Removing first ID variable so that it does not interfer with ML Algorithms:
projTraining <- projTraining[c(-1)]
# Transformation 3: Remove the columns / Variables has too many NAs (keep only the variable > 60% threshold of NA’s)
subprojTraining <- projTraining #creating another subset to iterate in loop
for(i in 1:length(projTraining)) { #for every column in the training dataset
if( sum( is.na( projTraining[, i] ) ) /nrow(projTraining) >= .6 ) { # NAs > 60% of total observations)
for(j in 1:length(subprojTraining)) {
if( length( grep(names(projTraining[i]), names(subprojTraining)[j]) ) ==1)  { #if the columns are the same:
subprojTraining <- subprojTraining[ , -j] #Remove that column
}
}
}
}
#To check the new N?? of observations
dim(subprojTraining)
clean1 <- colnames(subprojTraining)
clean2 <- colnames(subprojTraining[, -58]) #already with classe column removed
projTesting <- projTesting[clean1]
testing <- testingdata[clean2]
#To check the new N?? of observations
dim(projTesting)
dim(testing)
dim(subprojTraining)
str(subprojTraining)
modFitB1 <- randomForest(classe ~. , data=projTraining)
library(randomForest)
modFitB1 <- randomForest(classe ~. , data=projTraining)
predictionsB1 <- predict(modFitB1, projTesting, type = "class")
modFit <- train(classe ~., method="rf", data=projTraining, trControl=trainControl(method='cv'), number=5, allowParallel=TRUE )
modFitB1 <- randomForest(classe ~. , data=subprojTraining)
predictionsB1 <- predict(modFitB1, projTesting, type = "class")
confusionMatrix(predictionsB1, projTesting$classe)
predictionsB2 <- predict(modFitB1, testingdata, type = "class")
confMatrix <- confusionMatrix(predictionsB1, projTesting$classe)
confMatrix
confMatrix$overall[1]
predictionsB2 <- predict(modFitB1, testingdata, type = "class")
predictionsB2 <- predict(modFitB1, testingdata)
NAindex <- apply(trainingdata,2,function(x) {sum(is.na(x))})
trainingdata <- trainingdata[,which(NAindex == 0)]
NAindex <- apply(testingdata,2,function(x) {sum(is.na(x))})
testingRaw <- testingdata[,which(NAindex == 0)]
v <- which(lapply(trainingdata, class) %in% "numeric")
preObj <-preProcess(trainingdata[,v],method=c('knnImpute', 'center', 'scale'))
trainingPredict <- predict(preObj, trainingdata[,v])
trainingdata$classe <- trainingdata$classe
testinPredict <-predict(preObj,testingdata[,v])
v <- which(lapply(trainingdata, class) %in% "numeric")
preObj <-preProcess(trainingdata[,v],method=c('knnImpute', 'center', 'scale'))
trainingPredict <- predict(preObj, trainingdata[,v])
trainingdata$classe <- trainingdata$classe
testinPredict <-predict(preObj,testingdata[,v])
rm(trainingdata)
rm(testingdata)
# Removing nerar Zero covariates
nzv <- nearZeroVar(trainingPredict,saveMetrics=TRUE)
trainingdata <- trainingPredict[,nzv$nzv==FALSE]
nzv <- nearZeroVar(testinPredict,saveMetrics=TRUE)
testingdata <- testinPredict[,nzv$nzv==FALSE]
inTrain <- createDataPartition(y=trainingdata$classe, p=0.6, list=FALSE)
projTraining <- trainingdata[inTrain, ]; projTesting <- trainingdata[-inTrain, ]
dim(projTraining); dim(projTesting)
v <- which(lapply(trainingdata, class) %in% "numeric")
preObj <-preProcess(trainingdata[,v],method=c('knnImpute', 'center', 'scale'))
trainingPredict <- predict(preObj, trainingdata[,v])
trainingPredict$classe <- trainingPredict$classe
testinPredict <-predict(preObj,testingdata[,v])
rm(trainingdata)
rm(testingdata)
nzv <- nearZeroVar(trainingPredict,saveMetrics=TRUE)
trainingdata <- trainingPredict[,nzv$nzv==FALSE]
nzv <- nearZeroVar(testinPredict,saveMetrics=TRUE)
testingdata <- testinPredict[,nzv$nzv==FALSE]
inTrain <- createDataPartition(y=trainingdata$classe, p=0.6, list=FALSE)
projTraining <- trainingdata[inTrain, ]; projTesting <- trainingdata[-inTrain, ]
dim(projTraining); dim(projTesting)
projTraining <- projTraining[c(-1)]
# Remove the columns / Variables has too many NAs (keep only the variable > 60% threshold of NA’s)
subprojTraining <- projTraining #creating another subset to iterate in loop
for(i in 1:length(projTraining)) { #for every column in the training dataset
if( sum( is.na( projTraining[, i] ) ) /nrow(projTraining) >= .6 ) { # NAs > 60% of total observations)
for(j in 1:length(subprojTraining)) {
if( length( grep(names(projTraining[i]), names(subprojTraining)[j]) ) ==1)  { #if the columns are the same:
subprojTraining <- subprojTraining[ , -j] #Remove that column
}
}
}
}
dim(subprojTraining)
str(subprojTraining)
clean1 <- colnames(subprojTraining)
clean2 <- colnames(subprojTraining[, -58]) #already with classe column removed
projTesting <- projTesting[clean1]
projTraining <- subprojTraining
testing <- testingdata[clean2]
#To check the new N?? of observations
dim(projTesting)
dim(testing)
modFitB1 <- randomForest(classe ~. , data=subprojTraining)
trainingPredict$classe
trainingdata = read.csv("./Data/pml-training.csv", header=TRUE, as.is = TRUE, stringsAsFactors = FALSE, sep=',', na.strings=c('NA','','#DIV/0!'))
dim(trainingdata)
summary(trainingdata$classe)
testingdata = read.csv("./Data/pml-testing.csv", header=TRUE, as.is = TRUE, stringsAsFactors = FALSE, sep=',', na.strings=c('NA','','#DIV/0!'))
# Cleaning the variables
NAindex <- apply(trainingdata,2,function(x) {sum(is.na(x))})
trainingdata <- trainingdata[,which(NAindex == 0)]
NAindex <- apply(testingdata,2,function(x) {sum(is.na(x))})
testingdata <- testingdata[,which(NAindex == 0)]
v <- which(lapply(trainingdata, class) %in% "numeric")
preObj <-preProcess(trainingdata[,v],method=c('knnImpute', 'center', 'scale'))
trainingPredict <- predict(preObj, trainingdata[,v])
trainingPredict$classe <- trainingPredict$classe
testinPredict <-predict(preObj,testingdata[,v])
trainingPredict$classe
v <- which(lapply(trainingdata, classe) %in% "numeric")
preObj <-preProcess(trainingdata[,v],method=c('knnImpute', 'center', 'scale'))
trainingPredict <- predict(preObj, trainingdata[,v])
trainingPredict$classe <- trainingPredict$classe
testinPredict <-predict(preObj,testingdata[,v])
trainingPredict$classe
trainingPredict
NAindex <- apply(trainingdata,2,function(x) {sum(is.na(x))})
trainingdata <- trainingdata[,which(NAindex == 0)]
NAindex <- apply(testingdata,2,function(x) {sum(is.na(x))})
testingRaw <- testingdata[,which(NAindex == 0)]
# Removing nerar Zero covariates
nzv <- nearZeroVar(trainingdata,saveMetrics=TRUE)
trainingdata <- trainingdata[,nzv$nzv==FALSE]
nzv <- nearZeroVar(testingdata,saveMetrics=TRUE)
testingdata <- testingdata[,nzv$nzv==FALSE]
# Partioning the training set into two
inTrain <- createDataPartition(y=trainingdata$classe, p=0.6, list=FALSE)
projTraining <- trainingdata[inTrain, ]; projTesting <- trainingdata[-inTrain, ]
dim(projTraining); dim(projTesting)
# Killing first column of Dataset - ID Removing first ID variable so that it does not interfer with ML Algorithms:
projTraining <- projTraining[c(-1)]
# Remove the columns / Variables has too many NAs (keep only the variable > 60% threshold of NA’s)
subprojTraining <- projTraining #creating another subset to iterate in loop
for(i in 1:length(projTraining)) { #for every column in the training dataset
if( sum( is.na( projTraining[, i] ) ) /nrow(projTraining) >= .6 ) { # NAs > 60% of total observations)
for(j in 1:length(subprojTraining)) {
if( length( grep(names(projTraining[i]), names(subprojTraining)[j]) ) ==1)  { #if the columns are the same:
subprojTraining <- subprojTraining[ , -j] #Remove that column
}
}
}
}
dim(subprojTraining)
str(subprojTraining)
clean1 <- colnames(subprojTraining)
clean2 <- colnames(subprojTraining[, -58]) #already with classe column removed
projTesting <- projTesting[clean1]
projTraining <- subprojTraining
testing <- testingdata[clean2]
#To check the new N?? of observations
dim(projTesting)
dim(testing)
modFitB1 <- randomForest(classe ~. , data=subprojTraining)
trainingdata = read.csv("./Data/pml-training.csv", na.strings = c("NA", ""))
dim(trainingdata)
summary(trainingdata$classe)
testingdata = read.csv("./Data/pml-testing.csv", na.strings = c("NA", ""))
nzv <- nearZeroVar(trainingdata,saveMetrics=TRUE)
trainingdata <- trainingdata[,nzv$nzv==FALSE]
nzv <- nearZeroVar(testingdata,saveMetrics=TRUE)
testingdata <- testingdata[,nzv$nzv==FALSE]
inTrain <- createDataPartition(y=trainingdata$classe, p=0.6, list=FALSE)
projTraining <- trainingdata[inTrain, ]; projTesting <- trainingdata[-inTrain, ]
dim(projTraining); dim(projTesting)
projTraining <- projTraining[c(-1)]
subprojTraining <- projTraining #creating another subset to iterate in loop
for(i in 1:length(projTraining)) { #for every column in the training dataset
if( sum( is.na( projTraining[, i] ) ) /nrow(projTraining) >= .6 ) { # NAs > 60% of total observations)
for(j in 1:length(subprojTraining)) {
if( length( grep(names(projTraining[i]), names(subprojTraining)[j]) ) ==1)  { #if the columns are the same:
subprojTraining <- subprojTraining[ , -j] #Remove that column
}
}
}
dim(subprojTraining)
str(subprojTraining)
clean1 <- colnames(subprojTraining)
clean2 <- colnames(subprojTraining[, -58]) #already with classe column removed
projTesting <- projTesting[clean1]
projTraining <- subprojTraining
testing <- testingdata[clean2]
#To check the new N?? of observations
dim(projTesting)
dim(testing)
dim(projTesting)
modFitB1 <- randomForest(classe ~. , data=subprojTraining)
inTrain <- createDataPartition(y=trainingdata$classe, p=0.6, list=FALSE)
projTraining <- trainingdata[inTrain, ]; projTesting <- trainingdata[-inTrain, ]
dim(projTraining); dim(projTesting)
nzv <- nearZeroVar(trainingdata,saveMetrics=TRUE)
trainingdata <- trainingdata[,nzv$nzv==FALSE]
nzv <- nearZeroVar(testingdata,saveMetrics=TRUE)
testingdata <- testingdata[,nzv$nzv==FALSE]
rm(list = ls())
